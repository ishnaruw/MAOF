# MAOF: Multi-Agent Observability Framework  
*Automating API Orchestration using Large Language Models*

--- 

## Overview

**MAOF (Multi-Agent Observability Framework)** is a modular system that explores how Large Language Models (LLMs) can discover, rank, and compose APIs based on **observability metrics** such as response time, throughput, and availability.
It combines Retrieval-Augmented Generation (RAG), TOPSIS-based QoS ranking, and multi-agent orchestration through AutoGen to enable transparent and performance-aware API automation.

---

## Architecture

MAOF organizes its components as modular agents in a transparent, observable pipeline.

```

User Query
â”‚
â–¼
[Retriever Agent] â”€â”€â”€â–º Selects relevant APIs from the catalog
â”‚
â–¼
[Ranker Agent] â”€â”€â”€â”€â”€â”€â–º Applies TOPSIS ranking using QoS metrics (rt_ms, tp_rps, availability)
â”‚
â–¼
[Planner Agent] â”€â”€â”€â”€â”€â–º Generates a coherent composition plan
â”‚
â–¼
[Coordinator Agent] â”€â–º Fuses results across multiple LLMs

```

---

## Repository Structure

```

MAOF/
â”œâ”€â”€ data/                   # API datasets and generated artifacts
â”‚   â”œâ”€â”€ raw/                # Original ToolBench / WSDream datasets
â”‚   â”œâ”€â”€ processed/          # Cleaned catalogs and capability tags
â”‚   â”œâ”€â”€ data_gen/           # Jupyter notebooks for data extraction
â”‚   â””â”€â”€ results/            # Generated API inventories
â”‚
â”œâ”€â”€ prompts/                # LLM instruction templates
â”‚   â”œâ”€â”€ retriever.md
â”‚   â”œâ”€â”€ ranker_topsis.md
â”‚   â””â”€â”€ planner.md
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tools/fetch_services.py     # JSONL loader and batch fetcher
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ retriever.py            # LLM-based candidate selection
â”‚   â”‚   â”œâ”€â”€ ranker_topsis.py        # TOPSIS QoS ranking
â”‚   â”‚   â””â”€â”€ planner.py              # Plan composition generator
â”‚   â”œâ”€â”€ core/topsis_verify.py       # Numeric TOPSIS verification
â”‚   â””â”€â”€ driver/run_autogen_pipeline.py  # Main pipeline script
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/               # Latest agent outputs (retriever, ranker, planner)
â”‚   â””â”€â”€ comparisons/        # Evaluation summaries and plots
â”‚
â”œâ”€â”€ runs/                   # Dated experimental runs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## Installation & Setup

### 1. Create environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
````

### 2. Configure environment variables

MAOF currently supports **Azure OpenAI** deployments (default: `gpt-4o-dspy`).

Create a `.env` file in the root with:

```bash
AZURE_OPENAI_API_KEY=your_azure_api_key
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-05-01-preview
AZURE_OPENAI_DEPLOYMENT=gpt-4o-dspy
```

Alternatively, export them directly:

```bash
export AZURE_OPENAI_API_KEY=...
export AZURE_OPENAI_ENDPOINT=...
```

---

## Running the Pipeline

Run the full LLM-based retrievalâ€“rankingâ€“planning pipeline:

```bash
python -m src.driver.run_autogen_pipeline
```

### Outputs

The pipeline saves intermediate and final outputs under:

```
results/logs/
â”œâ”€â”€ retriever_autogen.json   # Selected candidate APIs
â”œâ”€â”€ ranker_autogen.json      # Ranked APIs (TOPSIS)
â”œâ”€â”€ planner_autogen.json     # Generated orchestration plan
â””â”€â”€ topsis_verify.json       # Numeric verification of LLM scores
```

---

## Experiment Modes

MAOF supports four experimental configurations for evaluating retrieval and observability effects.

| Mode | Retrieval | QoS | Status | Description |
|------|------------|-----|---------|-------------|
| **1. noRAG_noQoS** | LLM retriever only | âœ— | Planned | Baseline without QoS ranking |
| **2. noRAG_QoS** | LLM retriever | âœ“ |  Implemented | Current pipeline with TOPSIS QoS ranking |
| **3. RAG_noQoS** | FAISS prefilter + LLM retriever | âœ— | Planned | Adds embedding prefiltering |
| **4. RAG_QoS** | FAISS prefilter + LLM retriever | âœ“ | Planned | Full hybrid RAG + QoS pipeline |

The current implementation runs **Mode 2 (noRAG_QoS)** using three LLM agents (Retriever, Ranker, Planner) in the AutoGen framework.


---

## Agents Overview

| Agent                            | Function                             | Key File                      | Notes                                        |
| -------------------------------- | ------------------------------------ | ----------------------------- | -------------------------------------------- |
| **Retriever Agent**              | Selects relevant APIs for user goal  | `src/agents/retriever.py`     | Uses LLM to filter JSON catalog              |
| **Ranker Agent**                 | Performs QoS-based scoring           | `src/agents/ranker_topsis.py` | Follows TOPSIS ranking logic                 |
| **Planner Agent**                | Composes selected APIs into workflow | `src/agents/planner.py`       | Produces JSON plan output                    |
| **Coordinator Agent (in progress)** | Aggregates ranked outputs from multiple LLMs | (to be added) | Planned for cross-model fusion and consensus scoring |


---

## Result Interpretation

* **retriever_autogen.json** â†’ Candidate APIs (`api_id`, `reason`)
* **ranker_autogen.json** â†’ TOPSIS results (`C`, `D_plus`, `D_minus`)
* **planner_autogen.json** â†’ Final orchestration plan (`step`, `api_id`, `why`)
* **topsis_verify.json** â†’ Numerical verification of LLM ranking

Higher `C` means closer to the ideal QoS point (fast, reliable, available).

---

## Evaluation (in progress)

MAOF supports multi-LLM evaluation across:

* **LLMs:** GPT-4o, Mistral, OpenAI GPT-4o-mini, and local TinyLlama
* **Metrics:**

  * Candidate overlap @k
  * Kendall Ï„ agreement (LLM vs numeric TOPSIS)
  * Plan completeness and logical order
  * RAG vs no-RAG improvement
  * QoS impact on ranking consistency

Coordinator fusion and multi-LLM comparison are under active development.

---

## Data Sources

* **ToolBench** (API capability metadata)
* **WSDream** (QoS measurements: latency, throughput, availability)
* **Custom curated catalogs** for cross-domain service discovery experiments

---

## Future Extensions

* Add **semantic RAG module** using FAISS/Chroma for pre-retrieval filtering
* Extend coordinator agent for **cross-LLM fusion and justification**
* Automate batch runs across 10+ user queries Ã— 3 models Ã— 4 modes
* Integrate **evaluation dashboards** (e.g., Streamlit or Jupyter notebooks)

---

<!-- ## ðŸ§‘â€ðŸ’» Citation

If you use this framework or datasets, please cite:

```
@research{Subramanian2025MAOF,
  title={MAOF: Multi-Agent Observability Framework for Service Discovery and Composition},
  author={Ishwarya Narayana Subramanian and Eyhab Al-Masri},
  year={2025},
  institution={University of Washington Tacoma}
}
```

---

## ðŸ“œ License

MIT License Â© 2025 Ishwarya Narayana Subramanian
See [LICENSE](LICENSE) for details.

--- -->

## Acknowledgments

* **Prof. Eyhab Al-Masri**, University of Washington Tacoma â€” ealmasri@uw.edu  
* Supported by the University of Washington Masterâ€™s in Computer Science & Systems program

---

**Researcher:** Ishwarya Narayana Subramanian (University of Washington Tacoma)  
Contact: ishnaruw@uw.edu

