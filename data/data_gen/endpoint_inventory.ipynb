{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a0c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolBench root: /Users/ishwaryapns/Documents/Thesis/MAOF/data/raw/toolbench/data/toolenv/tools\n",
      "Output dir: /Users/ishwaryapns/Documents/Thesis/MAOF/data/processed/api_catalog_sample_balanced\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Paths\n",
    "# Defines paths to ToolBench data and WS-DREAM QoS matrices. Also sets the output directory.\n",
    "from pathlib import Path\n",
    "import json, re, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TOOLBENCH_ROOT = Path(\"/Users/ishwaryapns/Documents/Thesis/MAOF/data/raw/toolbench/data/toolenv/tools\")\n",
    "WSD_RT = Path(\"../raw/wsdream/dataset1/rtMatrix.txt\")\n",
    "WSD_TP = Path(\"../raw/wsdream/dataset1/tpMatrix.txt\")\n",
    "\n",
    "OUT_DIR = Path(\"../processed/api_catalog_sample_balanced\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NO_QOS_OUT = OUT_DIR / \"api_repo.no_qos.jsonl\"\n",
    "WITH_QOS_OUT = OUT_DIR / \"api_repo.with_qos.jsonl\"\n",
    "BALANCED_COUNTS = OUT_DIR / \"api_repo.balanced_counts.csv\"\n",
    "\n",
    "random.seed(42)\n",
    "print(\"ToolBench root:\", TOOLBENCH_ROOT.resolve())\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "545ddf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions and QoS Computation\n",
    "# Contains reusable functions to read files, parse JSON, extract endpoints, and compute QoS metrics.\n",
    "\n",
    "def read_matrix(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, header=None, sep=None, engine=\"python\")\n",
    "\n",
    "def slug(text: str) -> str:\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", (text or \"\").strip().lower()).strip(\"_\")\n",
    "    return s[:64] if s else \"x\"\n",
    "\n",
    "def safe_get(d, key, default=None):\n",
    "    return d.get(key, default) if isinstance(d, dict) else default\n",
    "\n",
    "def yield_endpoints_from_tool(tool_obj: dict, tool_base: str, category: str, file_name: str):\n",
    "    api_list = safe_get(tool_obj, \"api_list\", [])\n",
    "    if not isinstance(api_list, list): return\n",
    "    for ep in api_list:\n",
    "        if not isinstance(ep, dict): continue\n",
    "        name = safe_get(ep, \"name\") or \"\"\n",
    "        url = safe_get(ep, \"url\") or \"\"\n",
    "        method = safe_get(ep, \"method\") or \"\"\n",
    "        desc = safe_get(ep, \"description\") or \"\"\n",
    "        ep_id = f\"{tool_base}_{slug(name) or 'endpoint'}\"\n",
    "        yield {\n",
    "            \"api_id\": ep_id,\n",
    "            \"category\": category,\n",
    "            \"name\": name,\n",
    "            \"description\": desc,\n",
    "            \"method\": method,\n",
    "            \"url\": url,\n",
    "            \"_file\": file_name,\n",
    "            \"_tool\": tool_base,\n",
    "        }\n",
    "\n",
    "def iter_endpoints_any_json(json_path: Path, category: str):\n",
    "    try:\n",
    "        data = json.loads(json_path.read_text())\n",
    "    except Exception:\n",
    "        return\n",
    "    file_stem = json_path.stem\n",
    "    tool_base_default = slug(file_stem)\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"api_list\"), list):\n",
    "        yield from yield_endpoints_from_tool(data, tool_base_default, category, json_path.name)\n",
    "    elif isinstance(data, dict) and isinstance(data.get(\"tools\"), list):\n",
    "        for t in data[\"tools\"]:\n",
    "            if isinstance(t, dict):\n",
    "                tool_base = safe_get(t, \"standardized_name\") or safe_get(t, \"name\") or tool_base_default\n",
    "                yield from yield_endpoints_from_tool(t, slug(tool_base), category, json_path.name)\n",
    "    elif isinstance(data, dict) and isinstance(data.get(\"endpoints\"), list):\n",
    "        tool_like = {\n",
    "            \"api_list\": data[\"endpoints\"],\n",
    "            \"standardized_name\": safe_get(data, \"standardized_name\") or tool_base_default\n",
    "        }\n",
    "        yield from yield_endpoints_from_tool(tool_like, slug(tool_like[\"standardized_name\"]), category, json_path.name)\n",
    "    elif isinstance(data, list):\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, dict) and isinstance(item.get(\"api_list\"), list):\n",
    "                tool_base = safe_get(item, \"standardized_name\") or safe_get(item, \"name\") or f\"{tool_base_default}_{idx}\"\n",
    "                yield from yield_endpoints_from_tool(item, slug(tool_base), category, json_path.name)\n",
    "\n",
    "def compute_qos_per_column(rt_col: pd.Series, tp_col: pd.Series) -> dict:\n",
    "    rt_valid = rt_col.replace(-1, np.nan).dropna()\n",
    "    tp_valid = tp_col.replace(-1, np.nan).dropna()\n",
    "    availability = len(rt_valid) / len(rt_col) if len(rt_col) > 0 else 0.0\n",
    "    return {\n",
    "        \"rt_ms\": float(np.median(rt_valid)) if not rt_valid.empty else None,\n",
    "        \"tp_rps\": float(np.median(tp_valid)) if not tp_valid.empty else None,\n",
    "        \"availability\": round(availability, 4),\n",
    "        \"valid_qos\": not (rt_valid.empty or tp_valid.empty)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd86a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total endpoints collected: 49936\n",
      "Unique endpoints after de-duplication: 49936\n"
     ]
    }
   ],
   "source": [
    "all_endpoints = []\n",
    "category_dirs = sorted([p for p in TOOLBENCH_ROOT.iterdir() if p.is_dir()])\n",
    "\n",
    "for cat_dir in category_dirs:\n",
    "    category = cat_dir.name\n",
    "    for jf in sorted(cat_dir.glob(\"*.json\")):\n",
    "        for ep in iter_endpoints_any_json(jf, category):\n",
    "            all_endpoints.append(ep)\n",
    "\n",
    "print(f\"Total endpoints collected: {len(all_endpoints)}\")\n",
    "\n",
    "seen = {}\n",
    "unique_endpoints = []\n",
    "for ep in all_endpoints:\n",
    "    api_id = ep[\"api_id\"]\n",
    "    if api_id not in seen:\n",
    "        seen[api_id] = 1\n",
    "        unique_endpoints.append(ep)\n",
    "    else:\n",
    "        seen[api_id] += 1\n",
    "        new_id = f\"{api_id}-{seen[api_id]}\"\n",
    "        ep = {**ep, \"api_id\": new_id}\n",
    "        unique_endpoints.append(ep)\n",
    "\n",
    "print(f\"Unique endpoints after de-duplication: {len(unique_endpoints)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44634009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total balanced endpoints: 5604\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group endpoints by category\n",
    "cat_to_eps = defaultdict(list)\n",
    "for ep in unique_endpoints:\n",
    "    cat_to_eps[ep[\"category\"]].append(ep)\n",
    "\n",
    "# Count API quotas\n",
    "category_quota = {}\n",
    "fallback_categories = []\n",
    "target_total = 5825\n",
    "category_count = len(cat_to_eps)\n",
    "\n",
    "# First, assign max 118 quota to every category\n",
    "target_per_cat = target_total // category_count\n",
    "\n",
    "# Track categories with fewer endpoints than quota\n",
    "for cat, eps in cat_to_eps.items():\n",
    "    if len(eps) < target_per_cat:\n",
    "        category_quota[cat] = len(eps)\n",
    "        fallback_categories.append(cat)\n",
    "    else:\n",
    "        category_quota[cat] = target_per_cat\n",
    "\n",
    "# Compute remaining quota to redistribute\n",
    "allocated = sum(category_quota.values())\n",
    "remaining_quota = target_total - allocated\n",
    "remaining_cats = [cat for cat in cat_to_eps if cat not in fallback_categories]\n",
    "\n",
    "# Redistribute leftover quota evenly to remaining categories\n",
    "for cat in remaining_cats:\n",
    "    if remaining_quota <= 0:\n",
    "        break\n",
    "    category_quota[cat] += 1\n",
    "    remaining_quota -= 1\n",
    "\n",
    "# Final balanced endpoints list\n",
    "balanced_endpoints = []\n",
    "per_category_counts = []\n",
    "\n",
    "for cat, eps in cat_to_eps.items():\n",
    "    random.shuffle(eps)\n",
    "    selected_eps = eps[:category_quota[cat]]\n",
    "    balanced_endpoints.extend(selected_eps)\n",
    "    per_category_counts.append({\"category\": cat, \"selected\": len(selected_eps)})\n",
    "\n",
    "print(f\"Total balanced endpoints: {len(balanced_endpoints)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b6f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced category counts -> api_repo.balanced_counts.csv\n",
      "                                   category  selected\n",
      "0                               Advertising       118\n",
      "1  Artificial_Intelligence_Machine_Learning       118\n",
      "2                                  Business       118\n",
      "3                         Business_Software       118\n",
      "4                                  Commerce       118\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(unique_endpoints)\n",
    "quota = 5825\n",
    "per_cat_quota = quota // df[\"category\"].nunique()\n",
    "\n",
    "sampled = []\n",
    "counts = []\n",
    "for cat, group in df.groupby(\"category\"):\n",
    "    if len(group) <= per_cat_quota:\n",
    "        sampled_cat = group.copy()\n",
    "    else:\n",
    "        sampled_cat = group.sample(per_cat_quota, random_state=42)\n",
    "    sampled.append(sampled_cat)\n",
    "    counts.append({\"category\": cat, \"selected\": len(sampled_cat)})\n",
    "\n",
    "balanced_df = pd.concat(sampled).reset_index(drop=True)\n",
    "pd.DataFrame(counts).to_csv(BALANCED_COUNTS, index=False)\n",
    "\n",
    "print(f\"Saved balanced category counts -> {BALANCED_COUNTS.name}\")\n",
    "print(pd.DataFrame(counts).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28979d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS-DREAM matrix: 339 users × 5826 API columns\n"
     ]
    }
   ],
   "source": [
    "rt_df = read_matrix(WSD_RT)\n",
    "tp_df = read_matrix(WSD_TP)\n",
    "\n",
    "if rt_df.shape != tp_df.shape:\n",
    "    raise ValueError(f\"RT/TP shape mismatch: {rt_df.shape} vs {tp_df.shape}\")\n",
    "\n",
    "n_users, n_cols = rt_df.shape\n",
    "print(f\"WS-DREAM matrix: {n_users} users × {n_cols} API columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b84400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: api_repo.no_qos.jsonl  (5559 entries)\n",
      "Wrote: api_repo.with_qos.jsonl (5559 entries)\n"
     ]
    }
   ],
   "source": [
    "col_indices = list(range(n_cols))\n",
    "random.shuffle(col_indices)\n",
    "\n",
    "with NO_QOS_OUT.open(\"w\", encoding=\"utf-8\") as no_qos_f, \\\n",
    "     WITH_QOS_OUT.open(\"w\", encoding=\"utf-8\") as with_qos_f:\n",
    "\n",
    "    for i, ep in enumerate(balanced_df.to_dict(orient=\"records\")):\n",
    "        base = ep  # Keeping everything including _file for future use\n",
    "        no_qos_f.write(json.dumps(base, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        c = col_indices[i % n_cols]\n",
    "        qos = compute_qos_per_column(rt_df[c], tp_df[c])\n",
    "        with_qos_f.write(json.dumps({**base, \"qos\": qos}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote: {NO_QOS_OUT.name}  ({len(balanced_df)} entries)\")\n",
    "print(f\"Wrote: {WITH_QOS_OUT.name} ({len(balanced_df)} entries)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
